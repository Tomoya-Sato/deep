{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ソフトマックス回帰　(softmax regression)の実装\n",
    "# 目次\n",
    "1. 概要\n",
    "- 目標\n",
    "- 下準備\n",
    "- softmax関数の実装\n",
    "- 多クラス交差エントロピー誤差の実装\n",
    "- ソフトマックス回帰クラスの実装\n",
    "- 学習\n",
    "\n",
    "# 1. 概要\n",
    "- ロジスティック回帰の一般化で，多クラスに対応した手法\n",
    "- K個のクラス識別問題を考える．$\\boldsymbol{x}$：入力データ，$\\boldsymbol{t}$：教師データ\n",
    "\n",
    "\\begin{align}\n",
    "\\it{D}=\\left\\{\\left(\\boldsymbol{x}_i,\\boldsymbol{t}_i\\right)\\right\\}_{i=1}^{N}\\, ,\\boldsymbol{x}\\in\\mathbb{R}^d,\\, \\boldsymbol{t}\\in\\left\\{1,\\cdots,K\\right\\}\n",
    "\\end{align}\n",
    "\n",
    "- 各クラスの事後確率を求める．各クラスごとに重み行列 $\\boldsymbol{w}^{(𝑗)}$を持つ．\n",
    "$$\n",
    "P(y=1|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(1)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n",
    "P(y=2|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(2)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\\\\\n",
    "\\vdots\\\\\n",
    "P(y=K|\\,\\boldsymbol{x})=\\frac{\\exp({\\boldsymbol{w}^{(K)\\top}\\boldsymbol{x}})}{\\sum_{j=1}^{K}\\exp{(\\boldsymbol{w}^{(j)\\top}\\boldsymbol{x}})}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 目標\n",
    "- ソフトマックス回帰を実装して，mnist（手書き数字データセット）を識別する．\n",
    "- まず**活性化関数**の一種である**softmax**関数と，**交差エントロピー関数**を実装する．その後確率**的勾配降下法**を実装し，**SoftmaxRegression**クラスを実装する．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 下準備\n",
    "## 3.1 ライブラリのインポート\n",
    "- matplotlib: 図やグラフの描画など．\n",
    "- numpy: 行列演算など\n",
    "- sklearn: scikit-learn．様々な機械学習のモデルが利用できるが，今回はMNISTのデータをダウンロードするのに用いる．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from __future__ import print_function\n",
    "from test_softmax import *  # テスト用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 MNISTデータの読み込み\n",
    "- データをダウンロードする．一度ダウンロードすると，その後はデータを参照して読み込んでくれるので，毎回ダウンロードしなくても良くなる．\n",
    "- Xが画像データ，Yが正解データ\n",
    "- mnistのデータは，0~255のint型で表されているが，これを**255で割って**正規化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = fetch_mldata('MNIST original', data_home='./data/')\n",
    "X, Y = mnist.data, mnist.target\n",
    "X = X / 255.\n",
    "Y = Y.astype(\"int\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセットの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f344dcd0910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[i * 6500].reshape(28, 28), cmap='gray_r')\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習用・テスト用データの分割\n",
    "- 訓練データで学習し，同じ訓練データで性能の評価を行うと，訓練データでは良い性能を示すが，データを少しでも変えると性能が低下してしまうことがある（**過学習**）．\n",
    "- よって，学習する訓練データとは異なるテストデータで性能評価を行う．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(X, Y, test_size=0.2, random_state=2)\n",
    "train_y = np.eye(10)[train_y].astype(np.int32)\n",
    "test_y = np.eye(10)[test_y].astype(np.int32)\n",
    "train_n = train_x.shape[0]\n",
    "test_n = test_x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. softmax関数の実装\n",
    "活性化関数の一種であるsoftmax関数を実装する．\n",
    "- 関数：softmax\n",
    "    - 入力：$\\boldsymbol{X}=(\\boldsymbol{x_1},\\boldsymbol{x_2},\\cdots,\\boldsymbol{x_N})^\\top\\in\\mathbb{R}^{N\\times K}$\n",
    "\n",
    "    - 出力：$\\boldsymbol{Y}=(\\boldsymbol{y_1},\\boldsymbol{y_2},\\cdots,\\boldsymbol{y_N})^\\top\\in\\mathbb{R}^{N\\times K},\\,\\,\\,y_{nk} = softmax(\\boldsymbol{x_n})_k$\n",
    "    - オーバーフローを防ぐために$\\boldsymbol{x}_n$の最大値を$\\boldsymbol{x}_n$自身から引く\n",
    "$$\n",
    "\\begin{align}\n",
    "softmax(\\boldsymbol{x})_k&= \\frac{\\exp (x_{k})} {\\Sigma_{i=1}^{K}{\\exp (x_{i})}}\\\\\n",
    "&=\\frac{\\exp (-x_{max})\\exp (x_{k})}{\\exp (-x_{max})\\Sigma_{i=1}^{K}{\\exp (x_{i})}}=\\frac{\\exp (x_{k}-x_{max})} {\\Sigma_{i=1}^{K}{\\exp (x_{i}-x_{max})}}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "ヒント\n",
    "</summary>\n",
    "<ol>\n",
    "    <li>最大値\n",
    "    <ul> \n",
    "        <li>```axis=1```を指定するとデータ$\\boldsymbol{x_n}$ごとに最大値を計算できる</li>\n",
    "        <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n",
    "    </ul></li>\n",
    "    <li>$\\exp$\n",
    "    <ul>\n",
    "    <li>```np.exp()```</li>\n",
    "    </ul></li>\n",
    "    <li>合計\n",
    "    <ul>\n",
    "    <li>```np.sum()```</li>\n",
    "    <li>```axis=1```を指定するとデータ$\\boldsymbol{x_n}$ごとに合計を計算できる</li>\n",
    "    <li>行列のshapeを変えたくない場合は，```keepdims=True```を指定する</li>\n",
    "    </ul></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    y = np.zeros(x.shape)\n",
    "    y = np.exp(x - np.amax(x, axis=1, keepdims=True)) / np.sum(np.exp(x - np.amax(x, axis=1, keepdims=True)), axis=1, keepdims=True)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト．以下のセルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok!\n"
     ]
    }
   ],
   "source": [
    "test_softmax(softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 多クラスの交差エントロピー誤差の実装\n",
    "- 関数：cross_entropy\n",
    "    - 入力： $Y=(\\boldsymbol{y}_1,\\boldsymbol{y}_2,\\cdots,\\boldsymbol{y}_N)^\\top\\in \\mathbb{R}^{N\\times K}$, \n",
    "$T=(\\boldsymbol{t}_1,\\boldsymbol{t}_2,\\cdots,\\boldsymbol{t}_N)^\\top\\in \\mathbb{R}^{N\\times K}$<br />\n",
    "$\\boldsymbol{y}_n$はソフトマックス関数の出力，$\\boldsymbol{t}_n$は教師ラベル(1-of-K表現)\n",
    "\n",
    "    - 出力： \n",
    "    $$L=-\\frac{1}{N}\\sum_{n=1}^N \\sum_i \\boldsymbol t_{n,i} \\log \\boldsymbol y_{n,i}\\in\\mathbb{R}^1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "ヒント\n",
    "</summary>\n",
    "<ol>\n",
    "    <li>$\\log$\n",
    "    <ul> \n",
    "        <li>```np.log()```</li>\n",
    "    </ul></li>\n",
    "    <li>合計\n",
    "    <ul>\n",
    "    <li>```np.sum()```</li>\n",
    "    </ul></li>\n",
    "    <li>平均\n",
    "    <ul>\n",
    "    <li>```np.mean()```</li>\n",
    "    </ul></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y, t):\n",
    "    L = - np.mean(np.sum(t * np.log(y), axis=1, keepdims=True), axis=0)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト．以下のセルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok!\n"
     ]
    }
   ],
   "source": [
    "test_cross_entropy(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. ソフトマックス回帰の実装\n",
    "ソフトマックス回帰クラスを実装する．\n",
    "## 6.1 勾配降下法の実装\n",
    "SoftmaxRegressionクラスのgradient_decent関数を実装してください．\n",
    "- 関数：gradient_descent  \n",
    "    - 入力：\n",
    "        - 学習データ： $\\boldsymbol{X}\\in\\mathbb{R}^{N\\times D}$\n",
    "        - 予測ラベル：$\\boldsymbol{Y}\\in\\mathbb{R}^{N\\times K}$\n",
    "        - 教師ラベル：$\\boldsymbol{T}\\in\\mathbb{R}^{N\\times K}$\n",
    "        - 学習率：$\\epsilon \\in \\mathbb{R}$\n",
    "    - 更新：\n",
    "        - 重みとバイアス $\\boldsymbol{W},\\,\\boldsymbol{b}$\n",
    "        \n",
    "- 勾配降下法: （$\\boldsymbol{W},\\boldsymbol{b}$：パラメータ，$\\epsilon$：学習率）\n",
    "$$\n",
    "\\boldsymbol{W}\\leftarrow\\boldsymbol{W}-\\epsilon\\nabla_{\\boldsymbol{W}}L\\\\\n",
    "\\boldsymbol{b}\\leftarrow\\boldsymbol{b}-\\epsilon\\nabla_{\\boldsymbol{b}}L\n",
    "$$\n",
    "- ソフトマックス回帰の勾配 :\n",
    "\\begin{align}\n",
    "\\nabla_{\\boldsymbol{W}}L&=\\frac{1}{N}\\boldsymbol{X}^\\top(\\boldsymbol{Y}-\\boldsymbol{T})\\\\\n",
    "\\nabla_{\\boldsymbol{b}}L&=\\frac{1}{N}(1,1,...,1)(\\boldsymbol{Y}-\\boldsymbol{T})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>\n",
    "ヒント\n",
    "</summary>\n",
    "<ol>\n",
    "    <li>行列の積\n",
    "    <ul> \n",
    "        <li>```np.dot()```</li>\n",
    "    </ul></li>\n",
    "    <li>合計\n",
    "    <ul>\n",
    "    <li>```np.sum()```</li>\n",
    "    </ul></li>\n",
    "</ol>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxRegression:\n",
    "    def __init__(self, n_in, n_out):\n",
    "        self.W = np.random.uniform(0.08, -0.08, (n_in, n_out)) #勾配の初期化\n",
    "        self.b = np.zeros(n_out) #バイアスの初期化\n",
    "        \n",
    "    def gradient_decent(self, X, Y, T, eps):\n",
    "        batchsize = X.shape[0]\n",
    "        self.W = self.W - eps * np.dot(X.T, (Y - T)) / batchsize\n",
    "        self.b = self.b - eps * np.sum((Y - T), axis=0) / batchsize\n",
    "        \n",
    "    def train(self, x, t, lr):\n",
    "        y = softmax(np.dot(x, self.W) + self.b) #予測\n",
    "        self.gradient_decent(x, y, t, lr) #パラメータの更新\n",
    "        loss = cross_entropy(y, t) #ロスの算出\n",
    "        return y, loss\n",
    "\n",
    "    def test(self, x, t):\n",
    "        y = softmax(np.dot(x, self.W) + self.b) #予測\n",
    "        loss = cross_entropy(y, t) #ロスの算出\n",
    "        return y, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テスト．以下のセルを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok!\n"
     ]
    }
   ],
   "source": [
    "test_gradient_decent(SoftmaxRegression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 学習\n",
    "## 7.1 モデルの初期化\n",
    "入力は784次元，出力は10次元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxRegression(784, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 ハイパーパラメータの設定\n",
    "- 学習epoch数は20\n",
    "    - epoch数とは，学習データを何回学習するかを表す数である．\n",
    "- バッチサイズは100\n",
    "    - ミニバッチとは少数のサンプルからなる集合である．\n",
    "- 学習率は1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epoch = 20\n",
    "batchsize = 100\n",
    "lr = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 学習\n",
    "交差エントロピー誤差を確率的勾配降下法を用いて最小化する．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 |　Train loss 0.417, accuracy 0.8792 |　Test loss 0.318, accuracy 0.9094\n",
      "epoch 1 |　Train loss 0.314, accuracy 0.9100 |　Test loss 0.366, accuracy 0.8897\n",
      "epoch 2 |　Train loss 0.302, accuracy 0.9140 |　Test loss 0.295, accuracy 0.9194\n",
      "epoch 3 |　Train loss 0.295, accuracy 0.9166 |　Test loss 0.295, accuracy 0.9196\n",
      "epoch 4 |　Train loss 0.288, accuracy 0.9189 |　Test loss 0.295, accuracy 0.9169\n",
      "epoch 5 |　Train loss 0.286, accuracy 0.9191 |　Test loss 0.296, accuracy 0.9202\n",
      "epoch 6 |　Train loss 0.282, accuracy 0.9199 |　Test loss 0.296, accuracy 0.9209\n",
      "epoch 7 |　Train loss 0.280, accuracy 0.9208 |　Test loss 0.290, accuracy 0.9186\n",
      "epoch 8 |　Train loss 0.279, accuracy 0.9208 |　Test loss 0.289, accuracy 0.9209\n",
      "epoch 9 |　Train loss 0.277, accuracy 0.9207 |　Test loss 0.296, accuracy 0.9188\n",
      "epoch 10 |　Train loss 0.275, accuracy 0.9230 |　Test loss 0.290, accuracy 0.9223\n",
      "epoch 11 |　Train loss 0.274, accuracy 0.9224 |　Test loss 0.323, accuracy 0.9080\n",
      "epoch 12 |　Train loss 0.273, accuracy 0.9229 |　Test loss 0.298, accuracy 0.9179\n",
      "epoch 13 |　Train loss 0.271, accuracy 0.9234 |　Test loss 0.300, accuracy 0.9178\n",
      "epoch 14 |　Train loss 0.272, accuracy 0.9232 |　Test loss 0.338, accuracy 0.9031\n",
      "epoch 15 |　Train loss 0.270, accuracy 0.9240 |　Test loss 0.286, accuracy 0.9230\n",
      "epoch 16 |　Train loss 0.268, accuracy 0.9248 |　Test loss 0.307, accuracy 0.9149\n",
      "epoch 17 |　Train loss 0.267, accuracy 0.9243 |　Test loss 0.311, accuracy 0.9146\n",
      "epoch 18 |　Train loss 0.267, accuracy 0.9247 |　Test loss 0.297, accuracy 0.9194\n",
      "epoch 19 |　Train loss 0.266, accuracy 0.9246 |　Test loss 0.304, accuracy 0.9161\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epoch):\n",
    "    print ('epoch %d |　' % epoch, end=\"\")\n",
    "    \n",
    "    # Training\n",
    "    sum_loss = 0\n",
    "    pred_label = []\n",
    "    perm = np.random.permutation(train_n) #ランダムに並び替える\n",
    "    \n",
    "    for i in range(0, train_n, batchsize): #ミニバッチごとに学習を行う\n",
    "        x = train_x[perm[i:i+batchsize]]\n",
    "        y = train_y[perm[i:i+batchsize]]\n",
    "        \n",
    "        pred, loss = model.train(x, y, lr)\n",
    "        sum_loss += loss * x.shape[0]\n",
    "        # pred には， (N, 10)の形で，画像が0~9の各数字のどれに分類されるかの事後確率が入っている\n",
    "        # そこで，最も大きい値をもつインデックスを取得することで，識別結果を得ることができる\n",
    "        pred_label.extend(pred.argmax(axis=1))\n",
    "\n",
    "    loss = sum_loss / train_n\n",
    "    # 正解率\n",
    "    accu = accuracy_score(pred_label, np.argmax(train_y[perm], axis=1))\n",
    "    print('Train loss %.3f, accuracy %.4f |　' %(loss, accu), end=\"\")\n",
    "    \n",
    "    \n",
    "    # Testing\n",
    "    sum_loss = 0\n",
    "    pred_label = []\n",
    "    \n",
    "    for i in range(0, test_n, batchsize):\n",
    "        x = test_x[i: i+batchsize]\n",
    "        y = test_y[i: i+batchsize]\n",
    "        \n",
    "        pred, loss = model.test(x, y)\n",
    "        sum_loss += loss * x.shape[0]\n",
    "        pred_label.extend(pred.argmax(axis=1))\n",
    "        \n",
    "    loss = sum_loss / test_n\n",
    "    \n",
    "    accu = accuracy_score(pred_label, np.argmax(test_y, axis=1))\n",
    "    print('Test loss %.3f, accuracy %.4f' %(loss, accu) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストの正解率が92%程度になると成功です．"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
